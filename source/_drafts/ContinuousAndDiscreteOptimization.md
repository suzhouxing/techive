---
title: ContinuousAndDiscreteOptimization
date: 2022-04-17 16:04:49
categories:
- 组合优化
tags:
- 优化
- 运筹优化
- 组合优化
- 连续优化
---
离散优化中，由于无法巧妙地求出目标函数下降最快的方向，局部搜索使用了相对笨的办法，即枚举所有变量所有可能的取值计算其目标函数值，再挑选下降最多的动作，改变一个变量的取值。这是一种脚踏实地的迭代模式。
连续优化中，由于知道目标函数的表达式，可对其进行求导从而得出目标函数下降最快的方向，梯度法直接朝下降最多的方向移动，所有变量取值同时改变。这是一种不拘小节的迭代模式。

但是，连续优化的梯度法就不会有离散优化的局部搜索的短视缺陷吗？答案显然是否定的，其核心问题在于步长。梯度法关注的是当前点的梯度方向，但只要挪动一小步，梯度方向可能就变了，我们甚至可能在与目标背道而驰！从这个角度来看，梯度法反而短视到了极点。
另一方面，一次修改所有变量的取值也许有助于
更重要的是，研究梯度法的人似乎特别钟情于收敛性和收敛速度分析。对于无约束凸优化（例如某些特殊形式的二次优化）这种有巧妙结构的问题，共轭梯度法等二阶方法的迭代次数可以做到与变量数相当！这时候进行收敛速度分析是有意义的。诚然，二次优化具有相对较强的表达能力，可以求解用拟人拟物方法（引力、静电力、弹力等）建模的优化问题，但在大多数应用场景下表达能力仍然有所欠缺。离开了特殊的性质，收敛的终点变成了局部最优而非全局最优，
例如，神经网络的训练很少使用二阶方法，一阶方法仍是主流。